import os
import time
import traceback
from collections import OrderedDict

import torch
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import autocast, GradScaler

from lib.train.trainers import BaseTrainer
from lib.train.admin import AverageMeter, StatValue, TensorboardWriter
import lib.utils.misc as misc

# Optional HF upload + plotting
from huggingface_hub import upload_file, HfFolder, upload_folder
import matplotlib.pyplot as plt


class LTRTrainer(BaseTrainer):
    """
    Full-featured trainer for SeqTrack (restores original behaviour) with:
      - phase-based checkpoint saving (local)
      - resume from previous checkpoint support
      - per-phase IoU plotting
      - optional upload to Hugging Face
      - mixed precision (AMP) support
      - restore IoU values across resumed checkpoints
    """

    def __init__(self, actor, loaders, optimizer, settings, lr_scheduler=None, use_amp=False):
        super().__init__(actor, loaders, optimizer, settings, lr_scheduler)

        # restore default params that original trainer relied on
        self._set_default_settings()

        # keep the loader->stats mapping (same as original)
        self.stats = OrderedDict({loader.name: None for loader in self.loaders})

        # Tensorboard on main process
        if settings.local_rank in [-1, 0]:
            tensorboard_writer_dir = os.path.join(self.settings.env.tensorboard_dir, self.settings.project_path)
            if not os.path.exists(tensorboard_writer_dir):
                os.makedirs(tensorboard_writer_dir, exist_ok=True)
            self.tensorboard_writer = TensorboardWriter(tensorboard_writer_dir, [l.name for l in loaders])

        self.move_data_to_gpu = getattr(settings, 'move_data_to_gpu', True)
        self.settings = settings

        # AMP
        self.use_amp = use_amp
        self.scaler = GradScaler() if use_amp else None

        # IoU and Loss bookkeeping
        self.iou_values = []  # append one value per epoch (if found in stats)
        self.loss_values = []  # append one value per epoch (if found in stats)
        self.repo_id = getattr(self.settings, "repo_id", None)
        self.phase_name = getattr(self.settings, "phase_name", "phase_1")
        self.resume_checkpoint = getattr(self.settings, "resume_checkpoint", None)
        # Removed skipped_epochs - we want to retrain for reproducible results

        # Phase directory resolution - integrate with base trainer's checkpoint system
        if hasattr(self, '_checkpoint_dir') and self._checkpoint_dir:
            # Use base trainer's checkpoint directory structure
            self.phase_dir = os.path.join(self._checkpoint_dir, self.phase_name)
        else:
            # Fallback to settings-based directory
            default_base = r"D:\College\Level 4\Semester 1\Image Processing\Assignments_Solve\VideoX\SeqTrack"
            workspace_base = getattr(self.settings, "save_dir", None) or getattr(self.settings.env, "workspace_dir", None) or default_base
            self.phase_dir = os.path.join(os.path.abspath(workspace_base), "checkpoints", self.phase_name)
        
        os.makedirs(self.phase_dir, exist_ok=True)

        # Logging file path
        if getattr(self.settings, 'log_file', None) is None:
            # Use the specific log filename requested
            self.settings.log_file = os.path.join(self.phase_dir, "seqtrack-seqtrack_b256.log")

        # If resume_checkpoint provided, attempt to load weights, optimizer, epoch, and iou_values
        if self.resume_checkpoint:
            try:
                if os.path.exists(self.resume_checkpoint):
                    print(f"üîÅ Resuming from checkpoint: {self.resume_checkpoint}")
                    try:
                        ckpt = torch.load(self.resume_checkpoint, map_location='cpu')
                        net = self.actor.net.module if hasattr(self.actor.net, 'module') else self.actor.net
                        
                        # Load network weights with better error handling
                        if 'net' in ckpt:
                            missing_keys, unexpected_keys = net.load_state_dict(ckpt['net'], strict=False)
                            if missing_keys:
                                print(f"‚ö†Ô∏è Missing keys in network: {missing_keys[:5]}...")  # Show first 5
                            if unexpected_keys:
                                print(f"‚ö†Ô∏è Unexpected keys in network: {unexpected_keys[:5]}...")  # Show first 5
                        elif 'state_dict' in ckpt:
                            missing_keys, unexpected_keys = net.load_state_dict(ckpt['state_dict'], strict=False)
                            if missing_keys:
                                print(f"‚ö†Ô∏è Missing keys in state_dict: {missing_keys[:5]}...")
                            if unexpected_keys:
                                print(f"‚ö†Ô∏è Unexpected keys in state_dict: {unexpected_keys[:5]}...")
                        elif 'model_state' in ckpt:
                            missing_keys, unexpected_keys = net.load_state_dict(ckpt['model_state'], strict=False)
                            if missing_keys:
                                print(f"‚ö†Ô∏è Missing keys in model_state: {missing_keys[:5]}...")
                            if unexpected_keys:
                                print(f"‚ö†Ô∏è Unexpected keys in model_state: {unexpected_keys[:5]}...")
                        else:
                            if os.path.isdir(self.resume_checkpoint):
                                self.load_state_dict(self.resume_checkpoint)

                        # Load optimizer state
                        if 'optimizer' in ckpt:
                            try:
                                self.optimizer.load_state_dict(ckpt['optimizer'])
                                print("‚úÖ Optimizer state restored")
                            except Exception as e:
                                print("‚ö†Ô∏è Could not load optimizer state:", e)

                        # Load epoch
                        if 'epoch' in ckpt:
                            self.epoch = ckpt.get('epoch', self.epoch)
                            print(f"[{self.phase_name}] ‚úÖ Resuming from epoch: {self.epoch}")
                            
                            # Set epoch in data samplers for reproducible data loading
                            for loader in self.loaders:
                                if isinstance(loader.sampler, DistributedSampler):
                                    loader.sampler.set_epoch(self.epoch)
                            print(f"[{self.phase_name}] ‚úÖ Data sampler epoch set for reproducible data loading")

                        # --- RESTORE IoU VALUES ---
                        if 'iou_values' in ckpt and ckpt['iou_values'] is not None:
                            self.iou_values = ckpt['iou_values']
                            print(f"[{self.phase_name}] ‚úÖ IoU values restored: {len(self.iou_values)} epochs of history")
                            print(f"   IoU values: {self.iou_values[:5]}..." if len(self.iou_values) > 5 else f"   IoU values: {self.iou_values}")
                            
                            # Note: We will retrain all epochs from resume point for reproducible results
                        else:
                            print("‚ö†Ô∏è No IoU values found in checkpoint, starting fresh")
                            self.iou_values = []

                        # --- RESTORE LOSS VALUES ---
                        if 'loss_values' in ckpt and ckpt['loss_values'] is not None:
                            self.loss_values = ckpt['loss_values']
                            print(f"[{self.phase_name}] ‚úÖ Loss values restored: {len(self.loss_values)} epochs of history")
                            print(f"   Loss values: {self.loss_values[:5]}..." if len(self.loss_values) > 5 else f"   Loss values: {self.loss_values}")
                        else:
                            print("‚ö†Ô∏è No Loss values found in checkpoint, starting fresh")
                            self.loss_values = []

                        # Load stats if available
                        if 'stats' in ckpt and ckpt['stats'] is not None:
                            self.stats = ckpt['stats']
                            print("‚úÖ Training stats restored")

                        # Load random state if available
                        if 'random_state' in ckpt and ckpt['random_state'] is not None:
                            import random
                            import numpy as np
                            random.setstate(ckpt['random_state']['python'])
                            np.random.set_state(ckpt['random_state']['numpy'])
                            torch.set_rng_state(ckpt['random_state']['torch'])
                            if torch.cuda.is_available():
                                torch.cuda.set_rng_state(ckpt['random_state']['torch_cuda'])
                            print("‚úÖ Random state restored for reproducible training")

                        print("‚úÖ Resume checkpoint loaded successfully")
                    except Exception as e:
                        try:
                            self.load_state_dict(self.resume_checkpoint)
                            print("‚úÖ Fallback: loaded using base trainer method")
                        except Exception as inner_e:
                            print("‚ö†Ô∏è Error while loading resume checkpoint:", e, inner_e)
                else:
                    print(f"‚ö†Ô∏è Resume checkpoint path does not exist: {self.resume_checkpoint}")
            except Exception as e:
                print("‚ö†Ô∏è Error while loading resume checkpoint:", e)

    def _set_default_settings(self):
        default = {
            'print_interval': 10,
            'print_stats': None,
            'description': '',
            'grad_clip_norm': getattr(self.settings, 'grad_clip_norm', 0),
        }
        for param, default_value in default.items():
            if getattr(self.settings, param, None) is None:
                setattr(self.settings, param, default_value)

    def cycle_dataset(self, loader):
        self.actor.train(loader.training)
        torch.set_grad_enabled(loader.training)
        self._init_timing()

        for i, data in enumerate(loader, 1):
            if self.move_data_to_gpu and isinstance(data, dict):
                data = data.to(self.device)

            data['epoch'] = self.epoch
            data['settings'] = self.settings

            if not self.use_amp:
                loss, stats = self.actor(data)
            else:
                with autocast():
                    loss, stats = self.actor(data)

            if loader.training:
                self.optimizer.zero_grad()
                if not self.use_amp:
                    loss.backward()
                    if getattr(self.settings, 'grad_clip_norm', 0) > 0:
                        torch.nn.utils.clip_grad_norm_(self.actor.net.parameters(), self.settings.grad_clip_norm)
                    self.optimizer.step()
                else:
                    self.scaler.scale(loss).backward()
                    if getattr(self.settings, 'grad_clip_norm', 0) > 0:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.actor.net.parameters(), self.settings.grad_clip_norm)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()

            if torch.cuda.is_available():
                torch.cuda.synchronize()

            batch_size = data['template_images'].shape[loader.stack_dim] if 'template_images' in data else 1
            self._update_stats(stats, batch_size, loader)
            self._print_stats(i, loader, batch_size)

    def train_epoch(self):
        for loader in self.loaders:
            if self.epoch % loader.epoch_interval == 0:
                if isinstance(getattr(loader, 'sampler', None), DistributedSampler):
                    loader.sampler.set_epoch(self.epoch)
                self.cycle_dataset(loader)

        # Collect IoU and Loss BEFORE resetting meters
        # Always collect metrics for current epoch (will overwrite if resuming)
        iou_collected = False
        loss_collected = False
        
        for loader_name, loader_stats in self.stats.items():
            if loader_stats is None:
                continue
            for stat_name, stat_val in loader_stats.items():
                # Collect IoU values - try multiple patterns including exact match
                if not iou_collected and (stat_name.lower() == 'iou' or 
                    'iou' in stat_name.lower() or 
                    'intersection' in stat_name.lower()):
                    try:
                        # Only initialize if it doesn't exist, don't reset if it's None
                        if not hasattr(self, 'iou_values'):
                            self.iou_values = []
                        elif self.iou_values is None:
                            self.iou_values = []
                        
                        # Only append if we have a valid stat value
                        if hasattr(stat_val, 'avg') and stat_val.avg is not None:
                            iou_value = float(stat_val.avg)
                            self.iou_values.append(iou_value)
                            print(f"[{self.phase_name}] IoU collected: Epoch {self.epoch}, Value: {iou_value:.4f}, Total IoU history: {len(self.iou_values)} values")
                            iou_collected = True
                    except Exception as e:
                        print(f"Warning: Could not collect IoU value: {e}")
                        pass
                
                # Collect Loss values - try multiple patterns including exact match
                if not loss_collected and (stat_name.lower() == 'loss/total' or 
                    stat_name.lower() == 'loss' or 
                    'loss/total' in stat_name.lower() or 
                    ('loss' in stat_name.lower() and 'total' in stat_name.lower())):
                    try:
                        # Only initialize if it doesn't exist, don't reset if it's None
                        if not hasattr(self, 'loss_values'):
                            self.loss_values = []
                        elif self.loss_values is None:
                            self.loss_values = []
                        
                        # Only append if we have a valid stat value
                        if hasattr(stat_val, 'avg') and stat_val.avg is not None:
                            loss_value = float(stat_val.avg)
                            self.loss_values.append(loss_value)
                            print(f"[{self.phase_name}] Loss collected: Epoch {self.epoch}, Value: {loss_value:.4f}, Total Loss history: {len(self.loss_values)} values")
                            loss_collected = True
                    except Exception as e:
                        print(f"Warning: Could not collect Loss value: {e}")
                        pass
        
        # If no IoU was collected, try to extract from log file as fallback
        if not iou_collected:
            try:
                # This is a fallback method - extract IoU from the log file
                import re
                log_file = getattr(self.settings, 'log_file', None)
                if log_file and os.path.exists(log_file):
                    with open(log_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    # Look for the last IoU value in the log
                    for line in reversed(lines[-100:]):  # Check last 100 lines
                        if f'[{self.phase_name}]' in line and 'IoU:' in line:
                            iou_match = re.search(r'IoU:\s*([0-9.]+)', line)
                            if iou_match:
                                iou_value = float(iou_match.group(1))
                                if not hasattr(self, 'iou_values'):
                                    self.iou_values = []
                                elif self.iou_values is None:
                                    self.iou_values = []
                                self.iou_values.append(iou_value)
                                print(f"[{self.phase_name}] IoU collected (from log): Epoch {self.epoch}, Value: {iou_value:.4f}, Total IoU history: {len(self.iou_values)} values")
                                iou_collected = True
                                break
            except Exception as e:
                print(f"Warning: Could not extract IoU from log: {e}")
                pass

        self._stats_new_epoch()
        if self.settings.local_rank in [-1, 0]:
            self._write_tensorboard()

        # Save checkpoint and IoU plot
        if misc.is_main_process():
            try:
                # Use consistent naming with base trainer
                net = self.actor.net.module if hasattr(self.actor.net, 'module') else self.actor.net
                net_type = type(net).__name__
                ckpt_name = f"{net_type}_ep{self.epoch:04d}.pth.tar"
                ckpt_path = os.path.join(self.phase_dir, ckpt_name)

                # Include all necessary fields for proper resuming
                state = {
                    'epoch': self.epoch,
                    'actor_type': type(self.actor).__name__,
                    'net_type': net_type,
                    'net': net.state_dict(),
                    'net_info': getattr(net, 'info', None),
                    'constructor': getattr(net, 'constructor', None),
                    'optimizer': self.optimizer.state_dict(),
                    'stats': self.stats,
                    'iou_values': self.iou_values,  # SAVE IoU values for continuity
                    'loss_values': self.loss_values,  # SAVE Loss values for continuity
                    'settings': self.settings
                }
                
                # Save random state for reproducible resuming
                import random
                import numpy as np
                state['random_state'] = {
                    'python': random.getstate(),
                    'numpy': np.random.get_state(),
                    'torch': torch.get_rng_state(),
                    'torch_cuda': torch.cuda.get_rng_state() if torch.cuda.is_available() else None
                }

                # Atomic save operation
                tmp_path = ckpt_path + ".tmp"
                torch.save(state, tmp_path)
                if os.path.exists(ckpt_path):
                    os.remove(ckpt_path)
                os.rename(tmp_path, ckpt_path)

            except Exception as e:
                print("‚ö†Ô∏è Failed to save checkpoint:", e)
                traceback.print_exc()

            # IoU and Loss plots - show complete training history
            iou_fig_path = None
            loss_fig_path = None
            try:
                # IoU Plot
                if hasattr(self, 'iou_values') and len(self.iou_values) > 0:
                    iou_fig_path = os.path.join(self.phase_dir, f"{self.phase_name}_iou.png")
                    plt.figure(figsize=(10, 6))
                    
                    # Plot IoU values with proper epoch numbering
                    epochs = range(1, len(self.iou_values) + 1)
                    plt.plot(epochs, self.iou_values, marker='o', linewidth=2, markersize=4, color='blue')
                    
                    # Add current epoch marker
                    if len(self.iou_values) > 0:
                        plt.axvline(x=self.epoch, color='red', linestyle='--', alpha=0.7, 
                                  label=f'Current Epoch: {self.epoch}')
                    
                    plt.xlabel("Epoch")
                    plt.ylabel("IoU")
                    plt.title(f"IoU Progress - {self.phase_name} (Total Epochs: {len(self.iou_values)})")
                    plt.grid(True, alpha=0.3)
                    plt.legend()
                    plt.tight_layout()
                    plt.savefig(iou_fig_path, dpi=150, bbox_inches='tight')
                    plt.close()
                    print(f"[{self.phase_name}] IoU plot saved at: {iou_fig_path} (showing {len(self.iou_values)} epochs)")
                    print(f"[{self.phase_name}]    IoU values range: {min(self.iou_values):.4f} to {max(self.iou_values):.4f}")
                    print(f"[{self.phase_name}]    Current epoch: {self.epoch}, Total epochs in history: {len(self.iou_values)}")
                else:
                    print("NO IOU")
                    iou_fig_path = None

                # Loss Plot
                if hasattr(self, 'loss_values') and len(self.loss_values) > 0:
                    loss_fig_path = os.path.join(self.phase_dir, f"{self.phase_name}_loss.png")
                    plt.figure(figsize=(10, 6))
                    
                    # Plot Loss values with proper epoch numbering
                    epochs = range(1, len(self.loss_values) + 1)
                    plt.plot(epochs, self.loss_values, marker='o', linewidth=2, markersize=4, color='red')
                    
                    # Add current epoch marker
                    if len(self.loss_values) > 0:
                        plt.axvline(x=self.epoch, color='red', linestyle='--', alpha=0.7, 
                                  label=f'Current Epoch: {self.epoch}')
                    
                    plt.xlabel("Epoch")
                    plt.ylabel("Loss")
                    plt.title(f"Loss Progress - {self.phase_name} (Total Epochs: {len(self.loss_values)})")
                    plt.grid(True, alpha=0.3)
                    plt.legend()
                    plt.tight_layout()
                    plt.savefig(loss_fig_path, dpi=150, bbox_inches='tight')
                    plt.close()
                    print(f"[{self.phase_name}] Loss plot saved at: {loss_fig_path} (showing {len(self.loss_values)} epochs)")
                    print(f"[{self.phase_name}]    Loss values range: {min(self.loss_values):.4f} to {max(self.loss_values):.4f}")
                    print(f"[{self.phase_name}]    Current epoch: {self.epoch}, Total epochs in history: {len(self.loss_values)}")
                else:
                    loss_fig_path = None
            except Exception as e:
                print("‚ö†Ô∏è Failed to save plots:", e)
                iou_fig_path = None
                loss_fig_path = None


            # Optional Hugging Face upload
            if self.repo_id:
                try:
                    token = HfFolder.get_token()
                    if not token:
                        print("‚ö†Ô∏è Hugging Face token not found. Run `huggingface-cli login` first.")
                    else:
                        try:
                            upload_file(
                                path_or_fileobj=ckpt_path,
                                path_in_repo=f"{self.phase_name}/{os.path.basename(ckpt_path)}",
                                repo_id=self.repo_id,
                                repo_type="model",
                                token=token,
                            )
                            print(f"Uploaded checkpoint to Hugging Face: {self.repo_id}/{self.phase_name}")
                        except Exception as e:
                            print("‚ö†Ô∏è Failed uploading checkpoint to Hugging Face:", e)

                        # Upload IoU plot
                        if iou_fig_path:
                            try:
                                upload_file(
                                    path_or_fileobj=iou_fig_path,
                                    path_in_repo=f"{self.phase_name}/{os.path.basename(iou_fig_path)}",
                                    repo_id=self.repo_id,
                                    repo_type="model",
                                    token=token,
                                )
                                print(f"Uploaded IoU plot to Hugging Face: {self.repo_id}/{self.phase_name}")
                            except Exception as e:
                                print("‚ö†Ô∏è Failed uploading IoU plot to Hugging Face:", e)

                        # Upload Loss plot
                        if loss_fig_path:
                            try:
                                upload_file(
                                    path_or_fileobj=loss_fig_path,
                                    path_in_repo=f"{self.phase_name}/{os.path.basename(loss_fig_path)}",
                                    repo_id=self.repo_id,
                                    repo_type="model",
                                    token=token,
                                )
                                print(f"Uploaded Loss plot to Hugging Face: {self.repo_id}/{self.phase_name}")
                            except Exception as e:
                                print("‚ö†Ô∏è Failed uploading Loss plot to Hugging Face:", e)

                except Exception as e:
                    print("‚ö†Ô∏è Hugging Face upload block error:", e)

    # ----------------- Helpers copied / restored -----------------
    @staticmethod
    def _format_time(seconds):
        seconds = int(round(seconds))
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        secs = seconds % 60
        return f"{hours:01}:{minutes:02}:{secs:02} hours"

    def _init_timing(self):
        self.num_frames = 0
        self.start_time = time.time()
        self.prev_time = self.start_time
        self.last_log_time = self.start_time
        self.next_log_point = 50
        self.last_log_frames = 0

    def _update_stats(self, new_stats: OrderedDict, batch_size, loader):
        if loader.name not in self.stats.keys() or self.stats[loader.name] is None:
            self.stats[loader.name] = OrderedDict({name: AverageMeter() for name in new_stats.keys()})
        for name, val in new_stats.items():
            if name not in self.stats[loader.name].keys():
                self.stats[loader.name][name] = AverageMeter()
            self.stats[loader.name][name].update(val, batch_size)

    def _print_stats(self, i, loader, batch_size):
        self.num_frames += batch_size
        if self.num_frames >= self.next_log_point or i == loader.__len__():
            current_time = time.time()
            time_for_last_interval = current_time - self.last_log_time
            time_since_beginning = current_time - self.start_time

            total_samples = loader.__len__() * batch_size
            samples_left = max(0, total_samples - self.num_frames)
            avg_time_per_sample = time_since_beginning / self.num_frames if self.num_frames > 0 else 0
            time_left = samples_left * avg_time_per_sample

            log_str = (
                f"[{self.phase_name}] Epoch {self.epoch}: {self.num_frames} / {total_samples} samples,\n"
                f"time for last {self.num_frames - self.last_log_frames} samples: {self._format_time(time_for_last_interval)}\n"
                f"time since beginning {self._format_time(time_since_beginning)},\n"
                f"time left to finish the epoch {self._format_time(time_left)}\n"
            )

            stats_str = ""
            for name, val in self.stats[loader.name].items():
                if (self.settings.print_stats is None or name in self.settings.print_stats) and hasattr(val, 'avg'):
                    stats_str += f"{name}: {val.avg:.5f}, "
            log_str += stats_str.strip(', ') + "\n"

            print(log_str)
            if misc.is_main_process():
                try:
                    # Enhanced logging - save all training logs
                    with open(self.settings.log_file, 'a', encoding='utf-8') as f:
                        f.write(log_str)
                        f.flush()  # Ensure immediate write
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to write to log file: {e}")
                    pass

            self.last_log_time = current_time
            self.last_log_frames = self.num_frames
            self.next_log_point += 50

    def _stats_new_epoch(self):
        for loader in self.loaders:
            if loader.training:
                try:
                    lr_list = self.lr_scheduler.get_lr()
                except Exception:
                    try:
                        lr_list = self.lr_scheduler._get_lr(self.epoch)
                    except Exception:
                        lr_list = [None]
                for i, lr in enumerate(lr_list):
                    var_name = 'LearningRate/group{}'.format(i)
                    if var_name not in self.stats[loader.name].keys():
                        self.stats[loader.name][var_name] = StatValue()
                    self.stats[loader.name][var_name].update(lr)

        for loader_stats in self.stats.values():
            if loader_stats is None:
                continue
            for stat_value in loader_stats.values():
                if hasattr(stat_value, 'new_epoch'):
                    stat_value.new_epoch()

    def _write_tensorboard(self):
        if self.epoch == 1:
            try:
                self.tensorboard_writer.write_info(self.settings.script_name, self.settings.description)
            except Exception:
                pass
        try:
            self.tensorboard_writer.write_epoch(self.stats, self.epoch)
        except Exception:
            pass

    def train(self, max_epochs, load_latest=False, fail_safe=True, load_previous_ckpt=False, distill=False):
        """Override base trainer's train method to handle epoch skipping"""
        epoch = -1
        num_tries = 1
        for i in range(num_tries):
            try:
                if load_latest:
                    self.load_checkpoint()
                if load_previous_ckpt:
                    directory = '{}/{}'.format(self._checkpoint_dir, self.settings.project_path_prv)
                    self.load_state_dict(directory)
                if distill:
                    directory_teacher = '{}/{}'.format(self._checkpoint_dir, self.settings.project_path_teacher)
                    self.load_state_dict(directory_teacher, distill=True)
                
                # Modified training loop to skip already trained epochs
                for epoch in range(self.epoch+1, max_epochs+1):
                    self.epoch = epoch
                    
                    # No epoch skipping - retrain for reproducible results
                    
                    self.train_epoch()

                    if self.lr_scheduler is not None:
                        if self.settings.scheduler_type != 'cosine':
                            self.lr_scheduler.step()
                        else:
                            self.lr_scheduler.step(epoch - 1)
                    
                    # Save checkpoint at the end of every epoch
                    if self._checkpoint_dir:
                        if self.settings.local_rank in [-1, 0]:
                            self.save_checkpoint()
                            
            except:
                print(f'[{self.phase_name}] Training crashed at epoch {epoch}')
                if fail_safe:
                    self.epoch -= 1
                    load_latest = True
                    print('Traceback for the error!')
                    print(traceback.format_exc())
                    print(f'[{self.phase_name}] Restarting training from last epoch ...')
                else:
                    raise

        print('Finished training!')

    def save_checkpoint(self):
        """Override base trainer's save_checkpoint to use phase-specific directory"""
        # This method is called by the base trainer, but we handle saving in train_epoch
        # So we can either implement it here or let the base trainer handle it
        # For now, we'll implement a simple version that saves to phase directory
        if not misc.is_main_process():
            return
            
        try:
            net = self.actor.net.module if hasattr(self.actor.net, 'module') else self.actor.net
            net_type = type(net).__name__
            ckpt_name = f"{net_type}_ep{self.epoch:04d}.pth.tar"
            ckpt_path = os.path.join(self.phase_dir, ckpt_name)

            state = {
                'epoch': self.epoch,
                'actor_type': type(self.actor).__name__,
                'net_type': net_type,
                'net': net.state_dict(),
                'net_info': getattr(net, 'info', None),
                'constructor': getattr(net, 'constructor', None),
                'optimizer': self.optimizer.state_dict(),
                'stats': self.stats,
                'iou_values': self.iou_values,
                'loss_values': self.loss_values,
                'settings': self.settings
            }
            
            # Save random state for reproducible resuming
            import random
            import numpy as np
            state['random_state'] = {
                'python': random.getstate(),
                'numpy': np.random.get_state(),
                'torch': torch.get_rng_state(),
                'torch_cuda': torch.cuda.get_rng_state() if torch.cuda.is_available() else None
            }

            tmp_path = ckpt_path + ".tmp"
            torch.save(state, tmp_path)
            if os.path.exists(ckpt_path):
                os.remove(ckpt_path)
            os.rename(tmp_path, ckpt_path)
            
            print(f"Phase checkpoint saved at: {ckpt_path}")
        except Exception as e:
            print("‚ö†Ô∏è Failed to save phase checkpoint:", e)